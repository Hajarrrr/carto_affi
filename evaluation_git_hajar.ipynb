{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "79f43984-1f14-442e-b4d4-26f60062c678",
      "metadata": {
        "id": "79f43984-1f14-442e-b4d4-26f60062c678"
      },
      "source": [
        "# Evaluation - stage cartographie de l'écosystème\n",
        "\n",
        "Ce notebook est à réaliser chez soi dans un délai de 2 jours, mais rassurez-vous, ça ne prend pas autant de temps ! Il a pour objectif d'évaluer certaines de vos capacités en code et vos bonnes pratiques en scraping, NLP (Natural Language Processing) et manipulation d'API.\n",
        "\n",
        "L'objectif de ce notebook n'est pas de produire des outils opérationnels, mais d'évaluer vos compétences ou votre capacité à chercher des informations.\n",
        "\n",
        "En cas de doute ou de question, n'hésitez pas à nous écrire."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e601cb69",
      "metadata": {
        "id": "e601cb69"
      },
      "source": [
        "# 0. Git\n",
        "\n",
        "La première chose à faire est de \"récupérer\" ce notebook pour le compléter. Plusieurs options selon votre maîtrise de Git et/ou que vous ayez un compte :\n",
        "- fork le repo suivant et le compléter sur un projet perso\n",
        "- vous ajouter au projet, créez votre branch `submission_nomprenom` et push dessus\n",
        "- vous envoyez par mail le notebook à compléter et renvoyer par mail\n",
        "\n",
        "https://github.com/TitouanBlaize/carto_affi/blob/main/evaluation_git.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea047399-f5e2-4c73-888e-aeb969972f5c",
      "metadata": {
        "id": "ea047399-f5e2-4c73-888e-aeb969972f5c"
      },
      "source": [
        "## 1. NLP\n",
        "\n",
        "Cette première partie évalue vos connaissance des packages de NLP et vos bonnes pratiques quant au preprocessing de texte. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5929e7fb-77d9-4eb6-aae0-e87407b36678",
      "metadata": {
        "tags": [],
        "id": "5929e7fb-77d9-4eb6-aae0-e87407b36678"
      },
      "outputs": [],
      "source": [
        "# écrivez ici une liste de librairies que vous connaissez pour faire du NLP\n",
        "import spacy\n",
        "from spacy import displacy \n",
        "import fr_core_news_md"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eb7a5a7-b127-40b1-a502-a027b91f25d7",
      "metadata": {
        "id": "8eb7a5a7-b127-40b1-a502-a027b91f25d7"
      },
      "source": [
        "Dans une première sous-partie, nous vous proposons de coder un pipeline/fonction de preprocessing de données textuelles. \n",
        "\n",
        "L'objectif est, à partir de textes bruts, de produire des textes propres (ponctuation, casse, pluriel, enlever les stopwords,...) qui peuvent être utilisés en entrée d'un pipeline d'apprentissage/fonction. \n",
        "Libre à vous d'inclure et d'implémenter toutes les fonctions de nettoyage que vous souhaitez.\n",
        "Ces opérations peuvent être menées au sein d'une classe (cf class NLPPipeline) ou d'une fonction (NLP_cleaning), comme vous préférez."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fef2263-ac24-4943-8e30-7c7049f85f2f",
      "metadata": {
        "id": "2fef2263-ac24-4943-8e30-7c7049f85f2f"
      },
      "outputs": [],
      "source": [
        "# Via une classe (\"Orienté objet\")\n",
        "class NLPPipeline():\n",
        "    def __init__(self):\n",
        "      nlp = fr_core_news_md.load()\n",
        "      pass\n",
        "    \n",
        "    def transform(self, x):\n",
        "      for doc in nlp.pipe([x], disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
        "          print([(e.text, e.label_) for e in doc.ents])\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54064e06-0c87-465d-bfe8-80a1f5af35bd",
      "metadata": {
        "id": "54064e06-0c87-465d-bfe8-80a1f5af35bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1235d6b-427e-4f5d-90fd-c06174ae7a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Georges Washington', 'PERSON'), ('1er', 'ORDINAL'), ('USA', 'GPE'), ('Washington', 'GPE')]\n"
          ]
        }
      ],
      "source": [
        "pipeline = NLPPipeline()\n",
        "pipeline.transform(\"Georges Washington, 1er président des USA, habite à Washington.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "313ff353",
      "metadata": {
        "id": "313ff353"
      },
      "outputs": [],
      "source": [
        "# Via une fonction\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def NLP_cleaning(x):\n",
        "    final_str = \"\"\n",
        "    # lowercase\n",
        "    text = x.lower()\n",
        "    # supprimer le retour à la ligne\n",
        "    text = re.sub(r'\\n', '', text)\n",
        "    # Supprime la ponctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    # Supprimer les stop words\n",
        "    text = text.split()\n",
        "    useless_words = nltk.corpus.stopwords.words(\"french\")\n",
        "    text_filtered = [word for word in text if not word in useless_words]\n",
        "    # Supprimer les chiffres\n",
        "    text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
        "\n",
        "    final_str = ' '.join(text_filtered)\n",
        "    return final_str"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NLP_cleaning(\"Georges Washington, 1er président des USA, habite à Washington.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R5RXBxhsk-77",
        "outputId": "453f1a71-711a-40fb-f56f-10a83df75ad3"
      },
      "id": "R5RXBxhsk-77",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'georges washington  président usa habite washington'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67b90180",
      "metadata": {
        "id": "67b90180"
      },
      "source": [
        "On pourra par exemple essayer d'obtenir `georges washington er president usa habite a washington`, il n'y a pas de \"bon\" résultat, libre à vous d'ajouter plusieurs cleaning de textes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c342679-5a98-4e43-9260-deb132c6ba10",
      "metadata": {
        "id": "6c342679-5a98-4e43-9260-deb132c6ba10"
      },
      "source": [
        "## 2. Scraping/API\n",
        "\n",
        "Dans cette deuxième partie, nous vous proposons d'écrire une petite fonction de scraping.\n",
        "\n",
        "A l'aide de la librairie de votre choix, écrivez une fonction qui permet de trouver le nombre de citations d'un article sur Pubmed.   \n",
        "Vous appliquerez la fonction à l'article suivant: \"New onset refractory status epilepticus (NORSE) *C. Sculier, N. Gaspard*, 2019\" dont l'ID Pubmed est 30482654\n",
        "\n",
        "https://pubmed.ncbi.nlm.nih.gov/30482654/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "990bfcbf-7531-4b5d-98c4-df5610e2d206",
      "metadata": {
        "id": "990bfcbf-7531-4b5d-98c4-df5610e2d206"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup \n",
        "import requests "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b01bf9f-ea70-4a29-be37-3185b5eabf1e",
      "metadata": {
        "id": "8b01bf9f-ea70-4a29-be37-3185b5eabf1e"
      },
      "outputs": [],
      "source": [
        "def get_number_of_citations(pubmed_id):\n",
        "    \"\"\"\n",
        "    get the number of citations of a given pubmed article\n",
        "    \"\"\"\n",
        "    r = requests.get('https://pubmed.ncbi.nlm.nih.gov/?linkname=pubmed_pubmed_citedin&from_uid=' + str(pubmed_id)) \n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    all_results_amount = soup.find_all('div', attrs={'class': 'results-amount'})\n",
        "    #Test sur si l'aricle est cité ou pas\n",
        "    if all_results_amount:\n",
        "      print(int(all_results_amount[0].find_all('span', attrs={'class': 'value'})[0].text))\n",
        "    else:\n",
        "      print(0)\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb972d2",
      "metadata": {
        "id": "4cb972d2"
      },
      "source": [
        "En runnant la case suivante on est censé obtenir 19, bonne chance !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67337264-c9ad-4fed-85f2-f7ce96cb35ca",
      "metadata": {
        "id": "67337264-c9ad-4fed-85f2-f7ce96cb35ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab007e46-15e2-49c7-ec98-604594bcce8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19\n"
          ]
        }
      ],
      "source": [
        "get_number_of_citations(30482654)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cas d'aricle non cité\n",
        "get_number_of_citations(30482653)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V5--72Bp6xX",
        "outputId": "72f4dd0f-ad39-4327-8057-8efdef375bfc"
      },
      "id": "3V5--72Bp6xX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}